
# configuration groups
defaults:
  - encoder: hf_bert
  - train: biencoder_nq
  - datasets: table_train

train_datasets: [nq_table_train]
dev_datasets: [nq_table_dev]
output_dir:
train_sampling_rates:
loss_scale_factors:

# Whether to lower case the input text. Set True for uncased models, False for the cased ones.
do_lower_case: True

val_av_rank_start_epoch: 50   # 30
seed: 12345
checkpoint_file_name: 

# A trained bi-encoder checkpoint file to initialize the model
model_file:

# TODO: move to a conf group
# local_rank for distributed training on gpus

# TODO: rename to distributed_rank
local_rank: -1
global_loss_buf_sz: 592000
device:
distributed_world_size:
distributed_port:
distributed_init_method:

no_cuda: False
n_gpu:
fp16: False

# For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']."
#        "See details at https://nvidia.github.io/apex/amp.html
fp16_opt_level: O1

# tokens which won't be slit by tokenizer
special_tokens:

ignore_checkpoint_offset: False
ignore_checkpoint_optimizer: False
ignore_checkpoint_lr: False

# set to >1 to enable multiple query encoders
multi_q_encoder: False

# Set to True to reduce memory footprint and loose a bit the full train data randomization if you train in DDP mode
local_shards_dataloader: False


# preprocessing details 
row_selection: 'none'    # ['none', 'random', 'ngram']
max_cell_num: None        # maximum number of cells to include in the sequence

max_words: 120 
max_words_per_header: 12  # maximum number of words in the header cell 
max_words_per_cell: 8    # ~ in the content cells 
max_cell_num_per_row: 64   # default to no restrictions 
header_delimiter: '|' 
cell_delimiter: '|'
row_delimiter: '.'

max_sequence_length: 256 

structure_option: 'global'  # [global, rowcol, auxemb]